{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_model():\n",
    "    model = torch.load('./yolov7-w6-pose.pt', map_location=device)['model']\n",
    "    # Put in inference mode\n",
    "    model.float().eval()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # half() turns predictions into float16 tensors\n",
    "        # which significantly lowers inference time\n",
    "        model.half().to(device)\n",
    "    return model\n",
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(image):\n",
    "    # Resize and pad image\n",
    "    image = letterbox(image, 960, stride=64, auto=True)[0] # shape: (567, 960, 3)\n",
    "    # Apply transforms\n",
    "    image = transforms.ToTensor()(image) # torch.Size([3, 567, 960])\n",
    "    if torch.cuda.is_available():\n",
    "      image = image.half().to(device)\n",
    "    # Turn image into batch\n",
    "    image = image.unsqueeze(0) # torch.Size([1, 3, 567, 960])\n",
    "    with torch.no_grad():\n",
    "      output, _ = model(image)\n",
    "    return output, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints(output, image):\n",
    "  output = non_max_suppression_kpt(output, \n",
    "                                     0.25, # Confidence Threshold\n",
    "                                     0.65, # IoU Threshold\n",
    "                                     nc=model.yaml['nc'], # Number of Classes\n",
    "                                     nkpt=model.yaml['nkpt'], # Number of Keypoints\n",
    "                                     kpt_label=True)\n",
    "  with torch.no_grad():\n",
    "        output = output_to_keypoint(output)\n",
    "  nimg = image[0].permute(1, 2, 0) * 255\n",
    "  nimg = nimg.cpu().numpy().astype(np.uint8)\n",
    "  nimg = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n",
    "  for idx in range(output.shape[0]):\n",
    "      plot_skeleton_kpts(nimg, output[idx, 7:].T, 3)\n",
    "\n",
    "  return nimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_estimation_video(filename):\n",
    "    cap = cv2.VideoCapture(filename)\n",
    "    # VideoWriter for saving the video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    out = cv2.VideoWriter('./output/result.mp4', fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n",
    "    while cap.isOpened():\n",
    "        (ret, frame) = cap.read()\n",
    "        if ret == True:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            output, frame = run_inference(frame)\n",
    "            frame = draw_keypoints(output, frame)\n",
    "            frame = cv2.resize(frame, (int(cap.get(3)), int(cap.get(4))))\n",
    "            out.write(frame)\n",
    "            cv2.imshow('Pose estimation', frame)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    }
   ],
   "source": [
    "pose_estimation_video('../../testdata/human/video/yoga_3.mp4')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79ddc8254c70018a62c1f0c5fe5fc45576a7882643a05073dbdaa2a169de43e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
